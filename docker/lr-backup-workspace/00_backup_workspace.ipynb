{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup Workspace\n",
    "## Author: Jonn Smith\n",
    "## Date: 2023/02/28\n",
    "Detect all workspace data and tables, then download and back them up.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import firecloud.api as fapi\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "import lrmaCU.terra.table_utils as lrma_table_utils\n",
    "import lrmaCU.terra.workspace_utils as lrma_workspace_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track start time so we can calculate elapsed time for backup:\n",
    "START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "Set up our environment (Terra namespace, workspace, and the location of the bucket(s))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace: broad-firecloud-dsde-methods\n",
      "Workspace: sr-malaria\n",
      "Default Bucket: gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd\n"
     ]
    }
   ],
   "source": [
    "namespace = os.environ['WORKSPACE_NAMESPACE']\n",
    "workspace = os.environ['WORKSPACE_NAME']\n",
    "default_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "print(f\"Namespace: {namespace}\")\n",
    "print(f\"Workspace: {workspace}\")\n",
    "print(f\"Default Bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and dump our entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(namespace, workspace, table_name, store_membership=False):\n",
    "    ent_old = fapi.get_entities(namespace, workspace, table_name).json()\n",
    "    tbl_old = None\n",
    "\n",
    "    membership = None\n",
    "    if len(ent_old) > 0:\n",
    "        tbl_old = pd.DataFrame(list(map(lambda e: e['attributes'], ent_old)))\n",
    "        tbl_old[f\"entity:{table_name}_id\"] = list(map(lambda f: f['name'], ent_old))\n",
    "\n",
    "        if store_membership:\n",
    "            membership = list(map(lambda g: set(map(lambda h: h['entityName'], g['items'])), tbl_old['samples']))\n",
    "            del tbl_old['samples']\n",
    "\n",
    "        c = list(tbl_old.columns)\n",
    "        c.remove(f\"entity:{table_name}_id\")\n",
    "        c = [f\"entity:{table_name}_id\"] + c\n",
    "        tbl_old = tbl_old[c]\n",
    "        tbl_old = tbl_old.astype(str)\n",
    "\n",
    "    return tbl_old, membership\n",
    "\n",
    "# Remove any `nan` values in a given dataframe.\n",
    "# `nan` values are caused by a parsing issue and are artifacts.\n",
    "def fix_nans(df, quiet=True):\n",
    "    if not quiet: print(\"Replacing all `nan` values with empty strings: \")\n",
    "    for c in df.columns.values:\n",
    "        nan_types = (\"nan\", float('nan'))\n",
    "        has_nan = False\n",
    "        num_denaned = 0\n",
    "        for n in nan_types:\n",
    "            if (sum(df[c] == n) > 0):\n",
    "                num_denaned += sum(df[c] == n)\n",
    "                df.loc[df[c] == n, c] = \"\"\n",
    "                has_nan = True\n",
    "        if has_nan and not quiet:\n",
    "            print(f\"\\t{c}: {num_denaned}\")\n",
    "\n",
    "    if not quiet: print(\"Replacing numpy nan values...\")\n",
    "    if not quiet: print(\"Done.\")\n",
    "    return df.replace(np.nan, \"\")\n",
    "\n",
    "def _write_json_file_to_bucket(backup_folder_path, bucket, timestamp, namespace, workspace, json_object, file_base_name):\n",
    "    # Write our dict to our bucket:\n",
    "    blob = bucket.blob(f\"{backup_folder_path}/{timestamp}_{namespace}_{workspace}_{file_base_name}.json.gz\")\n",
    "    with blob.open('wb') as f:\n",
    "        f.write(gzip.compress(bytes(json.dumps(json_object, indent=4), 'utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our entity types so we know what to dump:\n",
    "entity_types = fapi.list_entity_types(namespace, workspace).json()\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "workspace_bucket = fapi.get_workspace(namespace, workspace).json()[\"workspace\"][\"bucketName\"]\n",
    "backup_folder_path = f\"backups/{timestamp}\"\n",
    "\n",
    "# Create our timestamped backup bucket:\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(workspace_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing workspace entities to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20240604T152727/tables\n",
      "\tsample_set_set\n",
      "\tz_external_pipeline_validation_set\n",
      "\tsample_set\n",
      "\tpfcrosses\n",
      "\tvalidation_set\n",
      "\tstandard_sample_set\n",
      "\tsample\n",
      "\tvalidation\n",
      "\tz_external_pipeline_validation\n",
      "\tremote_dataset\n",
      "\thigh_quality_assembly\n",
      "\tstandard_sample\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over entity types and dump each one to a separate TSV:\n",
    "print(f\"Writing workspace entities to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}/tables\")\n",
    "for et in entity_types:\n",
    "    print(f\"\\t{et}\")\n",
    "    tbl, _ = load_table(namespace, workspace, et)\n",
    "    tbl = fix_nans(tbl)\n",
    "    table_name = f\"{timestamp}_{namespace}_{workspace}_{et}.tsv.gz\"\n",
    "    \n",
    "    # Write our table to our bucket:\n",
    "    blob = bucket.blob(f\"{backup_folder_path}/tables/{table_name}\")\n",
    "    \n",
    "    with io.StringIO() as buf:\n",
    "        tbl.to_csv(buf, sep=\"\\t\", index=False)\n",
    "        with blob.open('wb') as f:\n",
    "            f.write(gzip.compress(bytes(buf.getvalue(), 'utf-8')))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing notebooks to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20240604T152727/notebooks\n",
      "\t00_backup_workspace.ipynb\n",
      "\t01_import_new_flowcell_data_from_tsv.ipynb\n",
      "\t02_add_new_flowcells_to_sample_table.ipynb\n",
      "\t10_hail_pca_analysis.ipynb\n",
      "\t98_rename_samples.ipynb\n",
      "\t99_Joint_Call_QC_And_Exploration.ipynb\n",
      "\t99_Submission_Statistics.ipynb\n",
      "\t99_fix_sample_table_bam_entries.ipynb\n",
      "\tZZ_99_workspace_scratch_notebook.ipynb\n",
      "\tbknight.ipynb\n",
      "\tdrug_resistance_heatmap.ipynb\n",
      "\thail_playground.ipynb\n",
      "\tinspect_joint_call_cohort_with_IGV.ipynb\n",
      "\tinspect_sample_with_IGV.ipynb\n",
      "\tkvg_quick_data_review.ipynb\n",
      "\treduceVCF.ipynb\n",
      "\tsz_00_backup_workspace.ipynb\n",
      "\tsz_base_template_for_all_routines_and_analysis.ipynb\n",
      "\tsz_copy_bai_to_data_table.ipynb\n",
      "\tsz_modify_data_table.ipynb\n",
      "\tsz_rename_samples.ipynb\n",
      "\tsz_using_beris_resource_monitoring.ipynb\n",
      "\tupdate_sample_table.ipynb\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Now backup the notebooks:\n",
    "print(\"Writing notebooks to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}/notebooks\")\n",
    "for notebook_blob in storage_client.list_blobs(workspace_bucket, prefix='notebooks'):\n",
    "    original_name = notebook_blob.name[notebook_blob.name.find(\"/\")+1:]\n",
    "    print(f\"\\t{original_name}\")\n",
    "    notebook_name = f\"{timestamp}_{namespace}_{workspace}_{original_name}\" \n",
    "    blob = bucket.copy_blob(notebook_blob, bucket, new_name=f\"{backup_folder_path}/notebooks/{notebook_name}\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing workspace attributes to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20240604T152727\n",
      "Done.\n",
      "\n",
      "Writing workspace metadata to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20240604T152727\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Now store workspace attributes:\n",
    "ws_dict = lrma_workspace_utils._query_workspace(namespace, workspace, 2)\n",
    "ws_attributes_dict = ws_dict['attributes']\n",
    "\n",
    "# Write our dict to our bucket:\n",
    "print(f\"Writing workspace attributes to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}\")\n",
    "\n",
    "_write_json_file_to_bucket(f\"{backup_folder_path}\", bucket, timestamp, namespace, workspace, ws_attributes_dict,\n",
    "                           f\"workspace_attributes\")\n",
    "\n",
    "print(f\"Done.\")\n",
    "print()\n",
    "\n",
    "# and store the workspace metadata:\n",
    "del ws_dict['attributes']\n",
    "\n",
    "print(f\"Writing workspace metadata to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}\")\n",
    "\n",
    "# Write our dict to our bucket:\n",
    "_write_json_file_to_bucket(f\"{backup_folder_path}\", bucket, timestamp, namespace, workspace, ws_dict,\n",
    "                           f\"workspace_metadata\")\n",
    "    \n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing workflow high-level information to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20240604T152727\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Now store workspace method (i.e. workflow) information:\n",
    "response = lrma_workspace_utils.retry_fiss_api_call('list_workspace_configs', 2, namespace, workspace, True)\n",
    "workflow_dict = response.json()\n",
    "\n",
    "# Sort our workflows in alphabetical order:\n",
    "workflow_dict = sorted(workflow_dict, key=lambda k: k['name'])\n",
    "\n",
    "print(f\"Writing workflow high-level information to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}\")\n",
    "\n",
    "# Write our dict to our bucket:\n",
    "_write_json_file_to_bucket(f\"{backup_folder_path}\", bucket, timestamp, namespace, workspace, workflow_dict,\n",
    "                               f\"workflows\")\n",
    "    \n",
    "print(f'Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing workflow metadata, inputs, and outputs to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20240604T152727/workflows\n",
      "\t01_SRFlowcell\n",
      "\t01_SRFlowcell_sz_standard_sample_id\n",
      "\t01_SRFlowcell_sz_standard_sample_id_increase_memory\n",
      "\t01_SRFlowcell_sz_standard_sample_id_increase_memory_sz_add_mem_disk_AlignedMetrics\n",
      "\t02_SRWholeGenome\n",
      "\t02_SRWholeGenome_jts_quick_hc_bugfix\n",
      "\t03_SRJointCallGVCFsWithGenomicsDB\n",
      "\t03_SRJointCallGVCFsWithGenomicsDB_jts_quick_hc_fix\n",
      "\t03_SRJointCallGVCFsWithGenomicsDB_sz_add_dsik_srjointgenotyping_from_jts_quick_hc_bugfix\n",
      "\t03_SRJointCallGVCFsWithGenomicsDB_test_Gnarly\n",
      "\t98_SRWholeGenome_Pf_Niare_VETS\n",
      "\t98_SRWholeGenome_Pf_Niare_VQSR\n",
      "\t99_SRJointCallGVCFsWithGenomicsDB_Pf_Niare_VETS\n",
      "\t99_SRJointCallGVCFsWithGenomicsDB_Pf_Niare_VQSR\n",
      "\tBenchmarkVCFs\n",
      "\tCompareVcfBenchmarks\n",
      "\tConvertToZarrStore\n",
      "\tDownloadFromSRA\n",
      "\tExpandedDrugResistanceMarkerAggregation\n",
      "\tExpandedDrugResistanceMarkerExtraction\n",
      "\tExtractRegionsFromBam\n",
      "\tGenerateMalariaReport\n",
      "\tHMMIBD_01_CLEAN_VCF\n",
      "\tONTPfHrp2Hrp3Status\n",
      "\tONTPfTypeDrugResistanceMarkers\n",
      "\tPfalciparumDrugResistanceSummary\n",
      "\tSRBamToFq\n",
      "\tSRIndexBam\n",
      "\tSRIndexBam_optional_outdir\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Get the metadata, default inputs, and default outputs for each workflow:\n",
    "\n",
    "print(f\"Writing workflow metadata, inputs, and outputs to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}/workflows\")\n",
    "\n",
    "for workflow_name in [w[\"name\"] for w in workflow_dict]:\n",
    "    print(f\"\\t{workflow_name}\")\n",
    "    \n",
    "    response = lrma_workspace_utils.retry_fiss_api_call('get_workspace_config', 2, namespace, workspace, namespace, workflow_name)\n",
    "    metadata = response.json()\n",
    "    \n",
    "    # Write the workflow metadata:\n",
    "    _write_json_file_to_bucket(f\"{backup_folder_path}/workflows\", bucket, timestamp, namespace, workspace, metadata,\n",
    "                               f\"{workflow_name}_workflow_metadata\")\n",
    "    \n",
    "    inputs = metadata['inputs']\n",
    "    outputs = metadata['outputs']\n",
    "    del metadata['inputs']\n",
    "    del metadata['outputs']\n",
    "    \n",
    "    # Write the workflow inputs:\n",
    "    _write_json_file_to_bucket(f\"{backup_folder_path}/workflows\", bucket, timestamp, namespace, workspace, inputs, \n",
    "                               f\"{workflow_name}_workflow_inputs\")\n",
    "    # Write the workflow outputs:\n",
    "    _write_json_file_to_bucket(f\"{backup_folder_path}/workflows\", bucket, timestamp, namespace, workspace, outputs,\n",
    "                               f\"{workflow_name}_workflow_outputs\")\n",
    "        \n",
    "print(f\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing workspace job history to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20240604T152727\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Finally store submissions:\n",
    "response = lrma_workspace_utils.retry_fiss_api_call('list_submissions', 2, namespace, workspace)\n",
    "submissions_dict = response.json()\n",
    "\n",
    "# Sort our submissions from most recent to least recent:\n",
    "submissions_dict = sorted(submissions_dict, key=lambda k: k['submissionDate'], reverse=True)\n",
    "\n",
    "print(f\"Writing workspace job history to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}\")\n",
    "\n",
    "_write_json_file_to_bucket(backup_folder_path, bucket, timestamp, namespace, workspace, submissions_dict, \"workspace_job_history\")\n",
    "    \n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track end time so we can calculate elapsed time for backup:\n",
    "END_TIME = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup completed on Tuesday June 04 at 11:28:34 ET\n",
      "Backup location: gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20240604T152727\n",
      "Elapsed time: 67.39s\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "now_utc = datetime.datetime.utcnow()\n",
    "timezone = pytz.timezone('America/New_York')\n",
    "now_et = now_utc.astimezone(timezone)\n",
    "time_string = now_et.strftime(\"%A %B %d at %H:%M:%S ET\")\n",
    "print(f\"Backup completed on {time_string}\")\n",
    "print(f\"Backup location: gs://{workspace_bucket}/{backup_folder_path}\")\n",
    "print(f\"Elapsed time: {END_TIME-START_TIME:2.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
