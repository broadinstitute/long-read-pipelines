version 1.0

import "../../../tasks/Utility/cnv_common_tasks.wdl" as CNVTasks

import "../../../tasks/Utility/Finalize.wdl" as FF

workflow LRProcessIntervalsForCnv {

    meta {
        description: "Preprocesses intervals for use with gCNV cohort mode."
        notes: ""
    }

    parameter_meta {
        intervals: "Required.  Intervals file (in .bed format) containing the regions to be analyzed.  For WGS, this should simply cover the chromosomes of interest.  For WES, this should cover the target regions to be analyzed."
        cohort_entity_id: "Required.  String identifier for the cohort used for denoising model generation."
        contig_ploidy_priors: "Required.  File containing reference contig ploidy priors."
        ref_map_file: "Required.  File containing the reference copy-number map."

        normal_bams: "Required.  Array of BAM files for the normal samples to be used for denoising model generation."
        normal_bais: "Required.  Array of BAI index files for the normal samples to be used for denoising model generation."

        blacklist_intervals: "Optional.  Intervals file (in .bed format) containing the regions to be excluded from analysis.  For WGS, this may be useful for excluding centromeric regions, etc. from analysis.  For WES, this may be useful for excluding regions that are not covered by the target capture kit."
        gatk_docker: "Optional.  Docker image for the GATK tool."

        gcs_out_root_dir: "Location to store outputs and metrics."
        run_uuid: "Used to distinguish different runs (e.g. diff. parameters)."
    }

    input {
        ##################################
        #### required basic arguments ####
        ##################################
        String gcs_out_root_dir
        String run_uuid

        Array[String]? tuning_readme_string

        ##################################
        #### required basic arguments ####
        ##################################
        File intervals
        File? blacklist_intervals
        Array[String]+ normal_bams
        Array[String]+ normal_bais
        String cohort_entity_id
        File contig_ploidy_priors
        File ref_map_file
        # Int num_intervals_per_scatter = 10000
        String gatk_docker = "broadinstitute/gatk:4.1.7.0"

        ##################################
        #### optional basic arguments ####
        ##################################
        # If true, AnnotateIntervals will be run to create GC annotations and explicit
        # GC correction will be performed by the model generated by
        Boolean? do_explicit_gc_correction
        File? gatk4_jar_override
        Int? preemptible_attempts

        ####################################################
        #### optional arguments for PreprocessIntervals ####
        ####################################################
        Int padding = 0
        Int bin_length = 50000

        ##################################################
        #### optional arguments for AnnotateIntervals ####
        ##################################################
        File? mappability_track_bed
        File? mappability_track_bed_idx
        File? segmental_duplication_track_bed
        File? segmental_duplication_track_bed_idx
        Int? feature_query_lookahead
        Int? mem_gb_for_annotate_intervals

        #################################################
        #### optional arguments for FilterIntervals ####
        ################################################
        File? blacklist_intervals_for_filter_intervals
        Float? minimum_gc_content
        Float? maximum_gc_content
        Float? minimum_mappability
        Float? maximum_mappability
        Float? minimum_segmental_duplication_content
        Float? maximum_segmental_duplication_content
        Int? low_count_filter_count_threshold
        Float? low_count_filter_percentage_of_samples
        Float? extreme_count_filter_minimum_percentile
        Float? extreme_count_filter_maximum_percentile
        Float? extreme_count_filter_percentage_of_samples
        Int? mem_gb_for_filter_intervals

        ##############################################
        #### optional arguments for CollectCounts ####
        ##############################################
        String? collect_counts_format
        Boolean? collect_counts_enable_indexing
        Int? mem_gb_for_collect_counts

        ########################################################################
        #### optional arguments for DetermineGermlineContigPloidyCohortMode ####
        ########################################################################
        Float? ploidy_mean_bias_standard_deviation
        Float? ploidy_mapping_error_rate
        Float? ploidy_global_psi_scale
        Float? ploidy_sample_psi_scale
        Int? mem_gb_for_determine_germline_contig_ploidy
        Int? cpu_for_determine_germline_contig_ploidy
    }

    #############################################################################
    String workflow_name = 'LRgCNV'
    #############################################################################

    Map[String, String] ref_map = read_map(ref_map_file)
    File ref_fasta_dict = ref_map['dict']
    File ref_fasta_fai = ref_map['fai']
    File ref_fasta = ref_map['fasta']

    call CNVTasks.PreprocessIntervals {
        input:
            intervals = intervals,
            blacklist_intervals = blacklist_intervals,
            ref_fasta = ref_fasta,
            ref_fasta_fai = ref_fasta_fai,
            ref_fasta_dict = ref_fasta_dict,
            padding = padding,
            bin_length = bin_length,
            gatk4_jar_override = gatk4_jar_override,
            gatk_docker = gatk_docker,
            preemptible_attempts = preemptible_attempts
    }

    if (select_first([do_explicit_gc_correction, true])) {
        call CNVTasks.AnnotateIntervals {
            input:
                intervals = PreprocessIntervals.preprocessed_intervals,
                ref_fasta = ref_fasta,
                ref_fasta_fai = ref_fasta_fai,
                ref_fasta_dict = ref_fasta_dict,
                mappability_track_bed = mappability_track_bed,
                mappability_track_bed_idx = mappability_track_bed_idx,
                segmental_duplication_track_bed = segmental_duplication_track_bed,
                segmental_duplication_track_bed_idx = segmental_duplication_track_bed_idx,
                feature_query_lookahead = feature_query_lookahead,
                gatk4_jar_override = gatk4_jar_override,
                gatk_docker = gatk_docker,
                mem_gb = mem_gb_for_annotate_intervals,
                preemptible_attempts = preemptible_attempts
        }
    }

    Array[Pair[String, String]] normal_bams_and_bais = zip(normal_bams, normal_bais)
    scatter (normal_bam_and_bai in normal_bams_and_bais) {
        call CNVTasks.CollectCounts {
            input:
                intervals = PreprocessIntervals.preprocessed_intervals,
                bam = normal_bam_and_bai.left,
                bam_idx = normal_bam_and_bai.right,
                ref_fasta = ref_fasta,
                ref_fasta_fai = ref_fasta_fai,
                ref_fasta_dict = ref_fasta_dict,
                format = collect_counts_format,
                enable_indexing = collect_counts_enable_indexing,
                gatk4_jar_override = gatk4_jar_override,
                gatk_docker = gatk_docker,
                mem_gb = mem_gb_for_collect_counts,
                preemptible_attempts = preemptible_attempts
        }
    }

    call CNVTasks.FilterIntervals {
        input:
            intervals = PreprocessIntervals.preprocessed_intervals,
            blacklist_intervals = blacklist_intervals_for_filter_intervals,
            annotated_intervals = AnnotateIntervals.annotated_intervals,
            read_count_files = CollectCounts.counts,
            minimum_gc_content = minimum_gc_content,
            maximum_gc_content = maximum_gc_content,
            minimum_mappability = minimum_mappability,
            maximum_mappability = maximum_mappability,
            minimum_segmental_duplication_content = minimum_segmental_duplication_content,
            maximum_segmental_duplication_content = maximum_segmental_duplication_content,
            low_count_filter_count_threshold = low_count_filter_count_threshold,
            low_count_filter_percentage_of_samples = low_count_filter_percentage_of_samples,
            extreme_count_filter_minimum_percentile = extreme_count_filter_minimum_percentile,
            extreme_count_filter_maximum_percentile = extreme_count_filter_maximum_percentile,
            extreme_count_filter_percentage_of_samples = extreme_count_filter_percentage_of_samples,
            gatk4_jar_override = gatk4_jar_override,
            gatk_docker = gatk_docker,
            mem_gb = mem_gb_for_filter_intervals,
            preemptible_attempts = preemptible_attempts
    }

    call DetermineGermlineContigPloidyCohortMode {
        input:
            cohort_entity_id = cohort_entity_id,
            intervals = FilterIntervals.filtered_intervals,
            read_count_files = CollectCounts.counts,
            contig_ploidy_priors = contig_ploidy_priors,
            gatk4_jar_override = gatk4_jar_override,
            gatk_docker = gatk_docker,
            mem_gb = mem_gb_for_determine_germline_contig_ploidy,
            cpu = cpu_for_determine_germline_contig_ploidy,
            mean_bias_standard_deviation = ploidy_mean_bias_standard_deviation,
            mapping_error_rate = ploidy_mapping_error_rate,
            global_psi_scale = ploidy_global_psi_scale,
            sample_psi_scale = ploidy_sample_psi_scale,
            preemptible_attempts = preemptible_attempts
    }

    if (defined(tuning_readme_string)) {
        call WriteReadMeString { input: readme_in_lines = select_first([tuning_readme_string]) }
    }

    #############################################################################
    String outdir = sub(gcs_out_root_dir, "/$", "") + "/~{workflow_name}"
    String run_specific_outdir = outdir + if (run_uuid == "") then "" else "/~{run_uuid}"

    call FF.FinalizeToFile as FF_PI { input:
        file = PreprocessIntervals.preprocessed_intervals, outdir = run_specific_outdir
    }
    if (select_first([do_explicit_gc_correction, true])) {
        call FF.FinalizeToFile as FF_AI { input:
            file = select_first([AnnotateIntervals.annotated_intervals]), outdir = run_specific_outdir
        }
    }
    call FF.FinalizeToFile as FF_FI { input:
        file = FilterIntervals.filtered_intervals, outdir = run_specific_outdir
    }
    call FF.FinalizeToDir as FF_RC { input:
        files = CollectCounts.counts, outdir = run_specific_outdir + "/ReadCounts/"
    }
    call FF.FinalizeToFile as FF_CPM { input:
        file = DetermineGermlineContigPloidyCohortMode.contig_ploidy_model_tar, outdir = run_specific_outdir
    }
    call FF.FinalizeToFile as FF_CPC { input:
        file = DetermineGermlineContigPloidyCohortMode.contig_ploidy_calls_tar, outdir = run_specific_outdir
    }

    if (defined(tuning_readme_string)) {
        call FF.FinalizeToFile as FF_readme { input:
            file = select_first([WriteReadMeString.readme]), outdir = run_specific_outdir
        }
    }

    output {
        Array[String] read_counts = FF_RC.final_paths
        Array[String] read_counts_entity_ids = CollectCounts.entity_id

        File preprocessed_intervals = FF_PI.gcs_path
        File? annotated_intervals = FF_AI.gcs_path
        File filtered_intervals = FF_FI.gcs_path

        File contig_ploidy_model_tar = FF_CPM.gcs_path
        File contig_ploidy_calls_tar = FF_CPC.gcs_path

        File? readme = FF_readme.gcs_path

        # Map[String, String] gcnv_output_summary = {
        #     "preprocessed_intervals":   FF_PI.gcs_path,
        #     "annotated_intervals":      select_first([FF_AI.gcs_path, "None"]),
        #     "filtered_intervals":       FF_FI.gcs_path,
        #     "read_counts":              FF_RC.gcs_dir
        # }
    }
}

task DetermineGermlineContigPloidyCohortMode {
    input {
      String cohort_entity_id
      File? intervals
      Array[File] read_count_files
      File contig_ploidy_priors
      String? output_dir
      File? gatk4_jar_override

      # Runtime parameters
      String gatk_docker
      Int? mem_gb
      Int? disk_space_gb
      Boolean use_ssd = false
      Int? cpu
      Int? preemptible_attempts

      # Model parameters
      Float? mean_bias_standard_deviation
      Float? mapping_error_rate
      Float? global_psi_scale
      Float? sample_psi_scale
    }

    # We do not expose Hybrid ADVI parameters -- the default values are decent

    Int machine_mem_mb = select_first([mem_gb, 7]) * 1000
    Int command_mem_mb = machine_mem_mb - 500

    # If optional output_dir not specified, use "out"
    String output_dir_ = select_first([output_dir, "out"])

    command <<<
        set -eu
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" gatk4_jar_override}
        export MKL_NUM_THREADS=~{default=8 cpu}
        export OMP_NUM_THREADS=~{default=8 cpu}

        gatk --java-options "-Xmx~{command_mem_mb}m"  DetermineGermlineContigPloidy \
            ~{"-L " + intervals} \
            --input ~{sep=" --input " read_count_files} \
            --contig-ploidy-priors ~{contig_ploidy_priors} \
            --interval-merging-rule OVERLAPPING_ONLY \
            --output ~{output_dir_} \
            --output-prefix ~{cohort_entity_id} \
            --verbosity DEBUG \
            --mean-bias-standard-deviation ~{default="0.01" mean_bias_standard_deviation} \
            --mapping-error-rate ~{default="0.01" mapping_error_rate} \
            --global-psi-scale ~{default="0.001" global_psi_scale} \
            --sample-psi-scale ~{default="0.0001" sample_psi_scale}

        tar czf ~{cohort_entity_id}-contig-ploidy-model.tar.gz -C ~{output_dir_}/~{cohort_entity_id}-model .
        tar czf ~{cohort_entity_id}-contig-ploidy-calls.tar.gz -C ~{output_dir_}/~{cohort_entity_id}-calls .
    >>>

    runtime {
        docker: gatk_docker
        memory: machine_mem_mb + " MB"
        disks: "local-disk " + select_first([disk_space_gb, 150]) + if use_ssd then " SSD" else " HDD"
        cpu: select_first([cpu, 8])
        preemptible: select_first([preemptible_attempts, 2])
    }

    output {
        File contig_ploidy_model_tar = "~{cohort_entity_id}-contig-ploidy-model.tar.gz"
        File contig_ploidy_calls_tar = "~{cohort_entity_id}-contig-ploidy-calls.tar.gz"
    }
}

task WriteReadMeString {
    input {
        Array[String] readme_in_lines
    }

    command <<<
        mv ~{write_lines(readme_in_lines)} "readme.txt"
    >>>

    output {
        File readme = "readme.txt"
    }

    runtime {
        disks: "local-disk 10 HDD"
        docker: "gcr.io/cloud-marketplace/google/ubuntu2004:latest"
    }
}
