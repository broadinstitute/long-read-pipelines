{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup Workspace\n",
    "## Author: Jonn Smith\n",
    "## Date: 2023/02/28\n",
    "Detect all workspace data and tables, then download and back them up.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import firecloud.api as fapi\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "Set up our environment (Terra namespace, workspace, and the location of the bucket(s))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace: broad-firecloud-dsde-methods\n",
      "Workspace: sr-malaria\n",
      "Default Bucket: gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd\n"
     ]
    }
   ],
   "source": [
    "namespace = os.environ['WORKSPACE_NAMESPACE']\n",
    "workspace = os.environ['WORKSPACE_NAME']\n",
    "default_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "print(f\"Namespace: {namespace}\")\n",
    "print(f\"Workspace: {workspace}\")\n",
    "print(f\"Default Bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and dump our entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(namespace, workspace, table_name, store_membership=False):\n",
    "    ent_old = fapi.get_entities(namespace, workspace, table_name).json()\n",
    "    tbl_old = None\n",
    "\n",
    "    membership = None\n",
    "    if len(ent_old) > 0:\n",
    "        tbl_old = pd.DataFrame(list(map(lambda e: e['attributes'], ent_old)))\n",
    "        tbl_old[f\"entity:{table_name}_id\"] = list(map(lambda f: f['name'], ent_old))\n",
    "\n",
    "        if store_membership:\n",
    "            membership = list(map(lambda g: set(map(lambda h: h['entityName'], g['items'])), tbl_old['samples']))\n",
    "            del tbl_old['samples']\n",
    "\n",
    "        c = list(tbl_old.columns)\n",
    "        c.remove(f\"entity:{table_name}_id\")\n",
    "        c = [f\"entity:{table_name}_id\"] + c\n",
    "        tbl_old = tbl_old[c]\n",
    "        tbl_old = tbl_old.astype(str)\n",
    "\n",
    "    return tbl_old, membership\n",
    "\n",
    "# Remove any `nan` values in a given dataframe.\n",
    "# `nan` values are caused by a parsing issue and are artifacts.\n",
    "def fix_nans(df, quiet=True):\n",
    "    if not quiet: print(\"Replacing all `nan` values with empty strings: \")\n",
    "    for c in df.columns.values:\n",
    "        nan_types = (\"nan\", float('nan'))\n",
    "        has_nan = False\n",
    "        num_denaned = 0\n",
    "        for n in nan_types:\n",
    "            if (sum(df[c] == n) > 0):\n",
    "                num_denaned += sum(df[c] == n)\n",
    "                df.loc[df[c] == n, c] = \"\"\n",
    "                has_nan = True\n",
    "        if has_nan and not quiet:\n",
    "            print(f\"\\t{c}: {num_denaned}\")\n",
    "\n",
    "    if not quiet: print(\"Replacing numpy nan values...\")\n",
    "    if not quiet: print(\"Done.\")\n",
    "    return df.replace(np.nan, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our entity types so we know what to dump:\n",
    "entity_types = fapi.list_entity_types(namespace, workspace).json()\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "workspace_bucket = fapi.get_workspace(namespace, workspace).json()[\"workspace\"][\"bucketName\"]\n",
    "backup_folder_path = f\"backups/{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create our timestamped backup bucket:\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.bucket(workspace_bucket)\n",
    "\n",
    "# # Iterate over entity types and dump each one to a separate TSV:\n",
    "# print(f\"Writing workspace entities to backup dir:\")\n",
    "# print(f\"gs://{workspace_bucket}/{backup_folder_path}\")\n",
    "# for et in entity_types:\n",
    "#     print(f\"\\t{et}\")\n",
    "#     tbl, _ = load_table(namespace, workspace, et)\n",
    "#     tbl = fix_nans(tbl)\n",
    "#     table_name = f\"{timestamp}_{namespace}_{workspace}_{et}.tsv\"\n",
    "    \n",
    "#     # Write our table to our bucket:\n",
    "#     blob = bucket.blob(f\"{backup_folder_path}/tables/{table_name}\")\n",
    "#     with blob.open('w') as f:\n",
    "#         tbl.to_csv(f, sep=\"\\t\", index=False)\n",
    "# print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing workspace entities to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20231218T153028\n",
      "\tsample_set_set\n",
      "\tz_external_pipeline_validation_set\n",
      "\thigh_quality_assembly\n",
      "\tsample_set\n",
      "\tpfcrosses\n",
      "\tvalidation_set\n",
      "\tsample\n",
      "\tvalidation\n",
      "\tz_external_pipeline_validation\n",
      "\tremote_dataset\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Create our timestamped backup bucket:\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(workspace_bucket)\n",
    "\n",
    "# Iterate over entity types and dump each one to a separate TSV:\n",
    "print(f\"Writing workspace entities to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}\")\n",
    "for et in entity_types:\n",
    "    print(f\"\\t{et}\")\n",
    "    tbl, _ = load_table(namespace, workspace, et)\n",
    "    tbl = fix_nans(tbl)\n",
    "    table_name = f\"{timestamp}_{namespace}_{workspace}_{et}.tsv.gz\"\n",
    "    \n",
    "    # Write our table to our bucket:\n",
    "    blob = bucket.blob(f\"{backup_folder_path}/tables/{table_name}\")\n",
    "    \n",
    "    with io.StringIO() as buf:\n",
    "        tbl.to_csv(buf, sep=\"\\t\", index=False)\n",
    "        with blob.open('wb') as f:\n",
    "            f.write(gzip.compress(bytes(buf.getvalue(), 'utf-8')))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing notebooks to backup dir:\n",
      "gs://fc-b06e896e-cc1d-4deb-b638-f7b87c3e5dbd/backups/20231218T153028\n",
      "\t00_backup_workspace.ipynb\n",
      "\t01_import_new_flowcell_data_from_tsv.ipynb\n",
      "\t02_add_new_flowcells_to_sample_table.ipynb\n",
      "\t10_hail_pca_analysis.ipynb\n",
      "\t99_fix_sample_table_bam_entries.ipynb\n",
      "\tbknight.ipynb\n",
      "\tdrug_resistance_heatmap.ipynb\n",
      "\thail_playground.ipynb\n",
      "\tinspect_joint_call_cohort_with_IGV.ipynb\n",
      "\tinspect_sample_with_IGV.ipynb\n",
      "\tupdate_sample_table.ipynb\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Now backup the notebooks:\n",
    "print(\"Writing notebooks to backup dir:\")\n",
    "print(f\"gs://{workspace_bucket}/{backup_folder_path}\")\n",
    "for notebook_blob in storage_client.list_blobs(workspace_bucket, prefix='notebooks'):\n",
    "    original_name = notebook_blob.name[notebook_blob.name.find(\"/\")+1:]\n",
    "    print(f\"\\t{original_name}\")\n",
    "    notebook_name = f\"{timestamp}_{namespace}_{workspace}_{original_name}\" \n",
    "    blob = bucket.copy_blob(blob, bucket, new_name=f\"{backup_folder_path}/notebooks/{notebook_name}\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup completed on Monday December 18 at 10:31:18 ET\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "now_utc = datetime.datetime.utcnow()\n",
    "timezone = pytz.timezone('America/New_York')\n",
    "now_et = now_utc.astimezone(timezone)\n",
    "time_string = now_et.strftime(\"%A %B %d at %H:%M:%S ET\")\n",
    "print(f\"Backup completed on {time_string}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
